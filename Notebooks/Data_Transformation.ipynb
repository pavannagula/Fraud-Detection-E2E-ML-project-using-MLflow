{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\pavan\\\\Desktop\\\\Fraud-Detection-E2E-ML-project-using-MLflow'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from fraud_detection_project import logger\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fraud_detection_project.constants import *\n",
    "from fraud_detection_project.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'], format='%d-%m-%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataTransformation:\n",
    "\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def read_data(data_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        It reads a CSV file and returns a Pandas DataFrame\n",
    "\n",
    "        Args:\n",
    "          file_path (str): The path to the file you want to read.\n",
    "\n",
    "        Returns:\n",
    "          A dataframe\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"reading the Dataset\")\n",
    "            return pd.read_csv(data_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(e)\n",
    "        \n",
    "\n",
    "    def extract_date_features(self, data_path):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            self.data: pd.DataFrame = DataTransformation.read_data(data_path)\n",
    "            logger.info(\"Deriving the Date features from Transactions Date\")\n",
    "            # Convert the date column to datetime format\n",
    "            self.data['trans_date_trans_time'] = pd.to_datetime(self.data['trans_date_trans_time'], format='%d-%m-%Y')\n",
    "            \n",
    "            # Extract month, year, and quarter\n",
    "            self.data['trans_month'] = self.data['trans_date_trans_time'].dt.month\n",
    "            self.data['trans_year'] = self.data['trans_date_trans_time'].dt.year\n",
    "            self.data['trans_quarter'] = self.data['trans_date_trans_time'].dt.quarter\n",
    "            \n",
    "            logger.info(\"Derived the new Date features from Transactions Date\")\n",
    "\n",
    "            return self.data \n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error:\", e)\n",
    "\n",
    "\n",
    "    def calculate_customer_age(self, data_path):\n",
    "       \n",
    "       try:\n",
    "            \n",
    "            self.data = self.extract_date_features(data_path)\n",
    "            \n",
    "            logger.info(\"Creating the Customer Age variable using DOB and Transactions Date\")\n",
    "            \n",
    "            self.data['dob'] = pd.to_datetime(self.data['dob'], format='%d-%m-%Y')\n",
    "            self.data['Cust_age'] = (self.data['trans_date_trans_time'] - self.data['dob']).dt.days // 365\n",
    "            \n",
    "            logger.info(\"Created the Customer Age variable\")\n",
    "\n",
    "            return self.data\n",
    "\n",
    "       except Exception as e:\n",
    "            raise Exception(\"Error:\", e)\n",
    "                        \n",
    "\n",
    "    def create_city_population_bins(self, data_path):\n",
    "        try:\n",
    "            \n",
    "            self.data = self.calculate_customer_age(data_path)\n",
    "\n",
    "            logger.info(\"Creating the city populations category by binning the city population\")\n",
    "\n",
    "            bins = [0, 5000, 50000, float('inf')]\n",
    "            labels = ['rural', 'sub-urban', 'urban']\n",
    "        \n",
    "            self.data['city_pop_category'] = pd.cut(self.data['city_pop'], bins=bins, labels=labels)\n",
    "\n",
    "            logger.info(\"Created the city_pop_category\")\n",
    "\n",
    "            return self.data\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error:\", e)\n",
    "        \n",
    "\n",
    "    def calculate_average_amount_by_category(self, data_path):\n",
    "\n",
    "        try:\n",
    "            \n",
    "            self.data = self.create_city_population_bins(data_path)\n",
    "\n",
    "            logger.info(\"Creating the avg_amount_by_category variable\")\n",
    "\n",
    "            avg_amount_by_category = self.data.groupby('category')['amt'].mean()\n",
    "            self.data['avg_amount_by_category'] = self.data['category'].map(avg_amount_by_category)\n",
    "            self.data = self.data.drop(labels=[\"trans_date_trans_time\", 'dob'], axis=1)\n",
    "            return self.data\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error\", e)\n",
    "    \n",
    "\n",
    "\n",
    "    def label_encoding(self, data_path):\n",
    "        try:\n",
    "            \n",
    "            self.data = self.calculate_average_amount_by_category(data_path)\n",
    "\n",
    "            logger.info(\"performing the Label encoding on Category and city_pop_category\")\n",
    "\n",
    "            encoded_data = self.data.copy()\n",
    "            columns = ['category', 'city_pop_category']\n",
    "            for col in columns:\n",
    "                encoder = LabelEncoder()\n",
    "                encoded_data[col] = encoder.fit_transform(self.data[col])\n",
    "            return encoded_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error\", e)\n",
    "        \n",
    "    def one_hot_encode_columns(self, data_path):\n",
    "        try:\n",
    "           \n",
    "            self.data = self.label_encoding(data_path)\n",
    "\n",
    "            logger.info(\"performing the one hot encoding on gender and trans_year\")\n",
    "            \n",
    "            cols = ['gender', 'trans_year']\n",
    "            encoded_data = self.data.copy()\n",
    "            encoded_data = pd.get_dummies(encoded_data, columns=cols, dtype=int)\n",
    "\n",
    "            logger.info(\"Done with feature encoding\")\n",
    "            \n",
    "            return encoded_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error\", e)\n",
    "\n",
    "\n",
    "    def fit_transform(self, data_path):\n",
    "        try:\n",
    "            \n",
    "            self.data = self.one_hot_encode_columns(data_path)\n",
    "\n",
    "            logger.info(\"performing feature scaling\")\n",
    "\n",
    "            columns_to_scale = ['amt', 'Cust_age', 'city_pop', 'avg_amount_by_category']\n",
    "            scaler = StandardScaler()\n",
    "            scaled_data = self.data.copy()\n",
    "            scaled_data[columns_to_scale] = scaler.fit_transform(self.data[columns_to_scale])\n",
    "\n",
    "            logger.info(\"Feature Scaling process is done\")\n",
    "        \n",
    "            return scaled_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error\", e)\n",
    "\n",
    "    def final_data(self, data_path):\n",
    "        try:\n",
    "            \n",
    "            self.data = self.fit_transform(data_path)\n",
    "\n",
    "            logger.info(\"Splitting Dataset into training and test sets Started\")\n",
    "            \n",
    "            # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "            train, test = train_test_split(self.data, test_size=0.2, random_state=42)\n",
    "\n",
    "            train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False)\n",
    "            test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\n",
    "\n",
    "            logger.info(\"Completed the split and stored Splited data into training and test sets\")\n",
    "            logger.info(train.shape)\n",
    "            logger.info(test.shape)\n",
    "\n",
    "            print(train.shape)\n",
    "            print(test.shape)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Error\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-25 01:05:51,756: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-08-25 01:05:51,756: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-08-25 01:05:51,772: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2023-08-25 01:05:51,772: INFO: common: created directory at: artifacts]\n",
      "[2023-08-25 01:05:51,772: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2023-08-25 01:05:51,787: INFO: 3578048169: reading the Dataset]\n",
      "[2023-08-25 01:05:52,960: INFO: 3578048169: Deriving the Date features from Transactions Date]\n",
      "[2023-08-25 01:05:53,336: INFO: 3578048169: Derived the new Date features from Transactions Date]\n",
      "[2023-08-25 01:05:53,336: INFO: 3578048169: Creating the Customer Age variable using DOB and Transactions Date]\n",
      "[2023-08-25 01:05:57,336: INFO: 3578048169: Created the Customer Age variable]\n",
      "[2023-08-25 01:05:57,336: INFO: 3578048169: Creating the city populations category by binning the city population]\n",
      "[2023-08-25 01:05:57,384: INFO: 3578048169: Created the city_pop_category]\n",
      "[2023-08-25 01:05:57,384: INFO: 3578048169: Creating the avg_amount_by_category variable]\n",
      "[2023-08-25 01:05:57,648: INFO: 3578048169: performing the Label encoding on Category and city_pop_category]\n",
      "[2023-08-25 01:05:58,170: INFO: 3578048169: performing the one hot encoding on gender and trans_year]\n",
      "[2023-08-25 01:05:58,435: INFO: 3578048169: Done with feature encoding]\n",
      "[2023-08-25 01:05:58,443: INFO: 3578048169: performing feature scaling]\n",
      "[2023-08-25 01:05:58,731: INFO: 3578048169: Feature Scaling process is done]\n",
      "[2023-08-25 01:05:58,741: INFO: 3578048169: Splitting Dataset into training and test sets Started]\n",
      "[2023-08-25 01:06:11,281: INFO: 3578048169: Completed the split and stored Splited data into training and test sets]\n",
      "[2023-08-25 01:06:11,281: INFO: 3578048169: (838860, 13)]\n",
      "[2023-08-25 01:06:11,281: INFO: 3578048169: (209715, 13)]\n",
      "(838860, 13)\n",
      "(209715, 13)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.final_data('artifacts/data_ingestion/fraud_data.csv')\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
